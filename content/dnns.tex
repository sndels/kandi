\section{Deep neural networks in tracking}
Trackers based on deep learning have been researched because of \ac{dnn}s' ability
to capture hierarchies of features from raw data with minimal earlier domain specific
knowledge. They provide greater versatility compared to traditional trackers that are 
based on hand-crafted sets of features. This chapter discusses effective trackers
that consist of \ac{dnn}s.

\subsection{Trackers using convolutional neural networks}

Multilayered \ac{cnn}s are currently common as feature extractors, but an early
implementation of a \ac{cnn}-based tracker~\cite{HUMAN_CNN} pre-dates the work of
Krizhevsky et.~al.~\cite{NIPS_IMAGENET} that sparked the current research on \ac{dnn}s
for classification tasks. Fan et.~al.~\cite{HUMAN_CNN} proposed a shift-variant architecture
utilizing the previous tracking result each frame. They recognized crowded scenes
containing multiple objects similar to the target to be especially challenging as false
positives could result in drift from the intended target. A reference position from the
previous frame was used to provide additional information for locating the target in the
current one. They also considered the shift-invariancy of conventional \ac{cnn}s to
present its own challenges. Shift-invariancy means that the position of an object does
not affect the network's output, which is beneficial for classification tasks as it is
desirable to recognize the objects in an image regardless of their position. However, a
tracker is expected to identify the location of the target, which motivated the adoption
of a shift-variant architecture.~\cite{HUMAN_CNN}

The tracker Fan et.~al.~\cite{HUMAN_CNN} presented had separate input layers for
extracting feature maps from the current and previous frame. These were then downsampled
together before splitting the rest of the convolutions into two branches to obtain both
global and local structures of features. Global structures were extracted via a series
of convolutions, while local structures were discovered by sampling the branching output
with a single layer of small kernels. The network's final layer took the outputs of both
branches and produced the final probability map based on them. Training was performed
on a set of 20,000 images obtained from surveilance videos and it was supervised as
the probabality map resulting from a pair of frames was compared to a target map. Online
traking was done on a fixed model to avoid the drift adaptive models can induce. The proposed
tracker perfomed especially when the target's position or the view changed as the tracker
was trained to track humans specifically.~\cite{HUMAN_CNN} Only being able to track a
single class of targets is a major limitation, but more recent trackers have used similar
approaches in conjuction with training on multiple different object classes.

\cite{MULTI_DOMAIN}
Learns branching and domain specific fully connected layers and the online tracking
substitutes them with a single new layer that is fine-tuned online with the shared
layers. Both long- and short-term updates are utilized to provide both robust and
adaptive tracking.

\cite{DEEPTRACK}
No pre-training as the network learns features online. Model is updated if training
loss is above a certain threshold.

\cite{DISCR_SALIENCY}
Combines a pre-trained feature descriptor \ac{cnn} and a SVM that creates a
saliency map from the extracted features. This map is used as a filter to extract
the position of the target in each frame.

\cite{SIAM_TRACK}
Applies a siamese architecture of two convolutional networks to object tracking.
A candidate image is compared to an exempla image and is scored based on their
similarities.

\cite{SPAT_RCN}
Combines an efficient feature extractor (YOLO) to spatial and temporal constraints.
The network's layers are first pre-trained with a traditional \ac{cnn} for general
feature learning. YOLO is then adopted as the detection module and the \ac{lstm}
is added before training it as part of the whole network. The \ac{lstm} is provide
robust access to long-range context and is fed with the output of the detection
stage converted to a 32x32 heatmap linked with the learned visual features.

\cite{FCN_TRACK_2}
Uses the conv4-3 and conv5-3 layers of the VGG network for selecting feature maps
that are fed to two different networks: a general network to capture category information
and a specific network to discriminate the target from the background. Both networks
output heatmaps which are used for final detection. The general network's result is used
by default while the specific network is used to determine the target location if
a distracter is detected in the background. Both networks are initialized on the first
frame, but only the specific network is updated online to avoid noise.

\cite{HIERARCH_FEATS}
Instead of using just the final output of a sequence of convolutional layers, the
proposed algorithm uses multiple layers to find the target's position. This is done
by going through the outputs coarse-to-fine to regularize the search for the maximum
value in the finer response maps. All the layers' correlation filter numerator and
denominator are also updated each frame to get a robust and computationally lighter
approximation of minimizing output error.

\cite{FCN_TRACK}
Views the traditionally fully connected layers at the end of a \ac{cnn} based
tracking network as convolutional layers and uses upscaling with skip connections
to previous layers. This \ac{fcn} is computationally lighter than a sliding window
based network as it only requires a single feedforward [connection?]. The network
was pre-trained on the VOC2012 dataset to learn features for targets of 20 categories
in the dataset. The tracker is only able to detect objects in those categories.
It also only allows single object tracking but the target can be identified in the
first probability map if the sequence contains multiple targets to permit multi-object
scenarios and increase accuracy in single-object tracking. (However, the method is
currently not efficient enough for tracking in real time.)

\subsection{Other approaches}
Not all trackers based on a \ac{dnn} use \ac{cnn}s for feature extraction. Wang and
Yeung~\cite{LEARNING_DEEP} proposed DLT, a tracker consisting of a pre-trained \ac{sdae}
and an additional classification layer. The \ac{sdae} was trained on the Tiny Images
dataset and its encoder part was used with a sigmoid classification layer for online
tracking.

\authorcomment{THIS IS USES CONVOLUTIONAL LAYERS FOR FEATURES, might be too shallow}
The ideas behind DLT were developed further by Wang et.~al.~\cite{LEARNED_HIERARCH}.
They observed that DLT couldn't obtain deep features with temporal invariance due to
training on unrelated images. Another remark was that DLT doesn't have an
integrated objective function to adapt the encoder to a target as the weights
are only updated if the target appearance seems to have changed during tracking.
The new feature learning method was integrated into an existing tracking system called
ASLSA~\cite{ASLSA}, which originally used raw pixel valus as its representations.

The two layer feature model learned features capable of handling complicated motion
transformations based on the work of Zou et.~al.~\cite{INVARIANT_FEATS}. It was trained
on auxiliary video sequences and the goal was to learn features invariant between two
frames, which results high-level features robust to non-linear motion patterns. These
generic features didn't include appearance information of specific target objects so a
domain adaptation module was added to both layers in order to to readjust them according
to a specific target.~\cite{LEARNED_HIERARCH}

Evaluation on some challenging sequences showed that the tracker Wang et.~al.~\cite{LEARNED_HIERARCH}
proposed was able to track targets that underwent non-rigid object deformation and in- or
out-of-plane rotations. It was also observed to perform better than the baseline
trackers and beat DLT in 5 of the 8 sequences tested. However, the tracker was implemented
as a single-threaded CPU program it only reached 0.6 fps in comparison to the 15 fps of
the GPU implementation of DLT~\cite{LEARNING_DEEP}.

\cite{SMS_DLT}
Uses a \ac{sdae} fine-tuned with SURF features gotten from matching the current frame's
to the first one's.

\cite{BLUR_TRACK}
Tackles the issue of motion blur as it is common in actual applications of object
tracking. Deblurring the images online is not computationally viable so the work
proposes a blur-invariant object tracker. It uses a deep hierarchial appearance
model pre-trained with unlabeled data that is blurred with varying kernel sizes
to make the model more robust. 