\section{Deep neural networks in tracking}
Trackers based on deep learning have been researched because of \ac{dnn}s' ability
to capture hierarchies of features from raw data with minimal earlier domain specific
knowledge. They provide greater versatility compared to traditional trackers that are
based on hand-crafted sets of features. This chapter reviews both early research
done in the field and more recent trackers that have made use of \ac{dnn}s.

\subsection{Early works}
Multilayered \ac{cnn}s are currently common as feature extractors, but an implementation
of a \ac{cnn}-based tracker~\cite{HUMAN_CNN} pre-dates the work of Krizhevsky et.
al.~\cite{NIPS_IMAGENET} that sparked the current research on \ac{dnn}s for classification
tasks. Fan et.~al.~\cite{HUMAN_CNN} proposed a shift-variant architecture utilizing the
previous tracking result each frame. They recognized crowded scenes containing multiple
objects similar to the target to be especially challenging as false positives could result
in drift from the intended target. A reference position from the previous frame was used
to provide additional information for locating the target in the current one. They also
considered the shift-invariancy of conventional \ac{cnn}s to present its own challenges.
Shift-invariancy means that the position of an object does not affect the network's output,
which is beneficial for classification tasks as it is desirable to recognize the objects
in an image regardless of their position. However, a tracker is expected to identify the
location of the target, which motivated the adoption of a shift-variant architecture.~\cite{HUMAN_CNN}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{human_cnn}
\caption{The network Fan et.~al.~\cite{HUMAN_CNN} used featured a branching desing to
         include two consecutive frames in the extracted features as well as to detect
         structures of features on both a local and global scale. Source: Fan et.
         al.~fig. 1~\cite{HUMAN_CNN}}\label{fig:human_cnn}
\end{figure}

The tracker Fan et.~al.~\cite{HUMAN_CNN} (fig.~\ref{fig:human_cnn}) presented had separate
input layers for extracting feature maps from the current and previous frame. These were
then downsampled together before splitting the rest of the convolutions into two branches
to obtain both global and local structures of features. Global structures were extracted
via a series of convolutions, while local structures were discovered by sampling the
branching output with a single convolution. The network's final layer took the outputs of
both branches and produced the final probability map based on them.~\cite{HUMAN_CNN}

Training was performed on a set of 20,000 images obtained from surveilance videos and it
was supervised as the probabality map resulting from a pair of frames was compared to a
target map. Online traking was done on a fixed model to avoid the drift adaptive models
can induce. The proposed tracker perfomed especially when the target's position or the
view changed as the tracker was trained to track humans specifically. Only being able
to track a single class of targets is a major limitation, but more recent trackers have
used similar approaches in conjuction with training on multiple different object classes.~\cite{HUMAN_CNN}

Not all trackers based on a \ac{dnn} use \ac{cnn}s for feature extraction. Wang and
Yeung~\cite{DLT} proposed DLT, a tracker consisting of a pre-trained \ac{sdae}
and an additional classification layer. The \ac{sdae} was trained on the Tiny Images
dataset and its encoder part was used with a sigmoid classification layer for online
tracking.

The ideas behind DLT were developed further by Wang et.~al.~\cite{LEARNED_HIERARCH}.
They observed that DLT couldn't obtain deep features with temporal invariance due to
training on unrelated images. Another remark was that DLT doesn't have an
integrated objective function to adapt the encoder to a target as the weights
are only updated if the target appearance seems to have changed during tracking.
The new \ac{cnn} based feature learning method was integrated into an existing tracking
system called ASLSA~\cite{ASLSA}, which originally used raw pixel valus as its representations.

The two layer feature model learned features capable of handling complicated motion
transformations based on the work of Zou et.~al.~\cite{INVARIANT_FEATS}. It was trained
on auxiliary video sequences and the goal was to learn features invariant between two
frames, which results high-level features strong against non-linear motion patterns. These
generic features didn't include appearance information of specific target objects so a
domain adaptation module was added to both layers in order to to readjust them according
to a specific target.~\cite{LEARNED_HIERARCH}

Evaluation on some challenging sequences showed that the tracker Wang et.~al.~\cite{LEARNED_HIERARCH}
proposed was able to track targets that underwent non-rigid object deformation and in- or
out-of-plane rotations. It was also observed to perform better than the baseline
trackers and beat DLT in 5 of the 8 sequences tested. However, the tracker was implemented
as a single-threaded CPU program and it only reached 0.6 frames/s in comparison to the 15
frames/s of the GPU implementation of DLT.~\cite{DLT}.

\subsection{Current state of the art}

The work of Ding et.~al.~\cite{BLUR_TRACK} focused on the issue of motion blur.
They noted that generic trackers make the assumption of having a blur free video
to work with, while motion blur is actually very common in real videos. It was stated
that the performance of such trackers may drop significantly if applied to videos with
severe motion blur and two challenges were mentioned for such situations: the appearance
features of the object are damaged by blur and abrupt motion of it is difficult to
estimate. Deblurring the input was dismissed as too computationally costly and potentially
prone to change its appearance features.~\cite{BLUR_TRACK}

A \ac{sdae} trained on blurred images was proposed as a solution for capturing blur-invariant
features. The layers were first pre-trained in sequence and in an unsupervised setting,
but fine-tuning of the model was done on all layers simultaneously and only the encoding
layers are used after this point.~\cite{BLUR_TRACK}

In online tracking, the model is initialised with 100 positive samples synthesized from the
initial bounding box by first selecting 10 bounds near it and warping them using geometric
transformations. This is done because initializing the model with only one positive sample
is not stable. The positive samples are also blurred using kernels simulating combinations
of different magnitudes and directions of blur to produce a model less affected by blur
effects. Not blurring the training samples might result in tracking failure or drift if
abrupt and severe motion blur occurs in the beginning of the sequence. The background is
sampled around the initial bounding box for negative samples and those are used without
transformations. Tracking results are stored during tracking to update the model if a
significant appearance change is detected by the maximum confidence dropping below a set
threshold.~\cite{BLUR_TRACK}

Evaluation was done in two parts. First, severely blurred videos were used to test
performance with severe blur and abrupt motion. After that, commonly used challenging
sequences were used to evaluate general performance in difficult conditions. The new
tracker performed well in both scenarios as it placed in the top two trackers in several
categories and good results were shown overall. Real-time tracking seems also feasible
as the tracker ran at 5--10 frames/s on a moderately powerful GPU and the implementation
left room for optimization.

\authorcomment{
\cite{MULTI_DOMAIN}
Learns branching and domain specific fully connected layers and the online tracking
substitutes them with a single new layer that is fine-tuned online with the shared
layers. Both long- and short-term updates are utilized to provide both robust and
adaptive tracking.

\cite{DEEPTRACK}
No pre-training as the network learns features online. Model is updated if training
loss is above a certain threshold.

\cite{DISCR_SALIENCY}
Combines a pre-trained feature descriptor \ac{cnn} and a SVM that creates a
saliency map from the extracted features. This map is used as a filter to extract
the position of the target in each frame.

\cite{SIAM_TRACK}
Applies a siamese architecture of two convolutional networks to object tracking.
A candidate image is compared to an exempla image and is scored based on their
similarities.

\cite{SPAT_RCN}
Combines an efficient feature extractor (YOLO) to spatial and temporal constraints.
The network's layers are first pre-trained with a traditional \ac{cnn} for general
feature learning. YOLO is then adopted as the detection module and the \ac{lstm}
is added before training it as part of the whole network. The \ac{lstm} is provide
robust access to long-range context and is fed with the output of the detection
stage converted to a 32x32 heatmap linked with the learned visual features.

\cite{FCN_TRACK_2}
Uses the conv4-3 and conv5-3 layers of the VGG network for selecting feature maps
that are fed to two different networks: a general network to capture category information
and a specific network to discriminate the target from the background. Both networks
output heatmaps which are used for final detection. The general network's result is used
by default while the specific network is used to determine the target location if
a distracter is detected in the background. Both networks are initialized on the first
frame, but only the specific network is updated online to avoid noise.

\cite{HIERARCH_FEATS}
Instead of using just the final output of a sequence of convolutional layers, the
proposed algorithm uses multiple layers to find the target's position. This is done
by going through the outputs coarse-to-fine to regularize the search for the maximum
value in the finer response maps. All the layers' correlation filter numerator and
denominator are also updated each frame to get a robust and computationally lighter
approximation of minimizing output error.

\cite{FCN_TRACK}
Views the traditionally fully connected layers at the end of a \ac{cnn} based
tracking network as convolutional layers and uses upscaling with skip connections
to previous layers. This \ac{fcn} is computationally lighter than a sliding window
based network as it only requires a single feedforward [connection?]. The network
was pre-trained on the VOC2012 dataset to learn features for targets of 20 categories
in the dataset. The tracker is only able to detect objects in those categories.
It also only allows single object tracking but the target can be identified in the
first probability map if the sequence contains multiple targets to permit multi-object
scenarios and increase accuracy in single-object tracking. (However, the method is
currently not efficient enough for tracking in real time.)

\cite{SMS_DLT}
Uses a \ac{sdae} fine-tuned with SURF features gotten from matching the current frame's
to the first one's.
}