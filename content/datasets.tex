\section{Datasets and evaluation}
\authorcomment{Overview of the datasets used for training and analysis. Methods
used for comparing performance.}
\authorcomment{Move these to section 3?}

\subsection{Datasets}
The datasets used for training are equally important as the actual network design
\authorcomment{citation}.
Research on networks working with image data has been made easier by larger sets of both
hand-labeled sets and ones obtained by simple keyword searches from online image services.
These kinds of sets can be used to pre-train useful target features to tracking networks.

VOC was a yearly competition for object recognition and VOC 2012 \cite{VOC12} is the last
challenge in the series. The datasets of the challenges are still used for pre-training
features for detection stages in tracking networks. There are four major subsets of
hand-labeled VOC data: classification, segmentation action classification, person layout.
Classification datasets consist of images annotated with the objects contained and bounding
boxes for the objects drawn in the image itself while image segmentation sets provide
additional mask images of the objects and classes in each shot. Action classification sets
contain descriptions and bounding boxes of actions the subjects are performing and person
layout sets contain bounding boxes for the subjects head, hands and feet.
\authorcomment{image examples of the datasets, descriptions to annotations}

The \ac{ilsvrc} \cite{ILSVRC15} is another recognition challenge running since 2010. The
most recent dataset consists of subsets of object localization, object detection and
object detection from video. The last subset is especially beneficial object tracking
tasks as it provides data for training on actual tracking data. The other two sets are also
substantially larger than the respective VOC sets as their labeling has been crowd sourced. 

There has also been an increase in resources solely devoted to tracking data with the
TB-100 -set~\cite{VTB} being a good example. It contains a hundred tracking sequences
with reference positions for the target on each frame. Because some of the targets are
similar or less challenging, a subset of 50 sequences considered challenging is also
provided as TB-50.~\cite{OT_BENCH}

The datasets used for the \ac{vot}~\cite{VOT} can also be used for training networks. The competition
is run yearly with updated evaluation sets which can be used for training as training
data for a network but the challenge itself prohibits training on tracking datasets for
participants.

There is also the yearly \ac{mot}~\cite{MOT16} for testing multiple object trackers but
its unique sequences can also be used to train single object trackers one object at a
time.

\subsection{Evaluation}
Evaluation of proposed trackers is a vital part of the research. It also limits the use
of annotated tracking sequences in training as training and evaluation should be done
on different data. The Visual Tracker Benchmark~\cite{VTB} is a commonly used resource
for comparing performance to other trackers. It consists of the TB-datasets, a code
library containing implementations of 31 publicly available trackers and ready benchmark
results for the included trackers. The code library is implemented using MATLAB and all
included trackers have been modified to use unified input and output formats. A
Python-based testing suite is also in development. The original benchmark was compiled
in 2015 so doesn't include more recent trackers in the suite.

The \ac{vot}~\cite{VOT} and \ac{mot}~\cite{MOT16} challenges also publish both the
yearly challenge suite and results, which can used to compare new networks against the
participants.
