\section{Background}

\subsection{Deep neural networks}

A \ac{dnn} is most commonly defined as a \ac{nn}, that has a \textbf{visible} input
and output layer with several \textbf{hidden layers} between them. The distinction
between visible and hidden layers is important because training only evaluates the
output layer's performance. During training, a learning algorithm optimizes the
individual hidden layers to best approximate the desired output of the whole network.

The input layer takes in the data to be processed, which typically means an array
of color values in the case of object tracking. These values are then processed
by the hidden layers and finally the output layer produces the target's position
in the frame. These models usually come in the form of a \textbf{Feedforward neural network}
or \textbf{\ac{mlp}}. The name comes from the fact that information flows from
the input, through computations, to the output with no \textbf{feedback} connections.
\\\authorcomment{picture from eg.\ deeplearningbook page 174?}

Each layer consist of several units with a weight and activation function. The weights
of a layer are commonly represented by a matrix by which the input is multiplied.
Simply put, a unit's activation function is fed by a sum of it's weighted inputs
and the result is output to the next layer. A commonly used unit type is the \ac{relu},
which is defined by the activation function $g (z) = \max\{0,z\}$. It provides a
nonlinear transformation while being comparable to linear models in terms of
generalizing well and being easy to optimize.
\authorcomment{how does training work}
\cite{DEEP_LEARNING}
\authorcomment{how to cite for the whole page? which isbn to use for the github-version?}
\subsection{Convolutional networks}

\authorcomment{what is a convolutional network, differences to a generic network}
\authorcomment{how do the layers function}
\authorcomment{benefits}
