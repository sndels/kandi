\section{Deep Learning}
Neural networks are heavily researched for numerous applications and they are loosely
based on the way a brain functions. The basic model a \ac{nn} consist of inputs, outputs
and a connecting layer of neurons. This chapter introduces the basic concepts behind
general deep neural networks, convolutional neural networks and stacked denoising
autoencoders.~\ref{ssec:dldnn} and~\ref{ssec:dlcnn} are based on the book Deep Learning by
Goodfellow et.~al.~\cite{DEEP_LEARNING}.

\subsection{Deep neural networks}\label{ssec:dldnn}

A \ac{dnn} is commonly defined as a \ac{nn} that has a \textbf{visible} input and
output layer with several \textbf{hidden layers} between them. The distinction between
visible and hidden layers is important because training of the network only evaluates
the output layer's performance. During training, a \textbf{learning algorithm} optimizes
the individual hidden layers to best approximate the desired output of the whole network.

The input layer takes in the data to be processed, which typically means a vector of
color values in the case of object tracking. These are then processed by the hidden
layers and finally the output layer produces the target's position in the frame. These
models usually come in the form of a \textbf{feedforward neural network} or
\textbf{\ac{mlp}}. The name comes from the fact that information flows from the input
through computations to the output with no \textbf{feedback} connections. Typically
this means that connections are only between consecutive layers.

In \ac{nn}s, each layer consist of several \textbf{units} with an activation function
and a weight for each of their input connections. The weights of the layer's inputs
are commonly represented by a matrix by which the input vector is multiplied as each
row represents a unit's input weights. All of the units can be connected to all of the
inputs (fig.\ref{fig:fcon}) forming a fully connected layer or just some of them
(fig.\ref{fig:scon}) utilizing sparse connections. Sparse layers can be implemented
by defining unique input vectors for the units. Units in a layer have a common activation
function that is fed by the sum of its weighted inputs. The \textbf{\ac{relu}} is a
commonly used unit type and is defined by the activation function $g (z) = \max\{0,z\}$.
It provides a nonlinear transformation while being comparable to linear models in terms
of generalizing well and being easy to optimize. A bias-term can also be defined for
each unit and a vector containing the layer's biases is summed to the outputs of the
activation function before passing the results to the next layer.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{dnn}
\caption{A fully connected network with two inputs \textit{i}, two hidden layers of
         four units \textit{h} and three outputs \textit{o}. Each set of connections
         is represented by a weight matrix \textbf{W} which indicates mapping from one
         layer to another. Excluding the input, all layers also have an activation
         function and their units can be assigned individual weights.}\label{fig:fcon}
\end{figure}

Before training, the weights of a \ac{mlp} are initialized to small random values and 
biases to zero or small positive values. Then an algorithm called \textbf{stochastic
gradient descent} is commonly applied alongside a training dataset. The basic procedure
is to calculate the error of the netwok's output values compared to the desired ones 
using a \textbf{loss function}. The function's gradient can then be calculated for
example by \textbf{back-porpagation}, which feeds the errors back through the network
to assign a contribution value to each unit. These values are then used to calculate
the gradient of the loss function relative to the weights. Each weight is adjusted
slightly to the opposite sign to minimize the loss function.

\subsection{Convolutional neural networks}\label{ssec:dlcnn}

A \ac{cnn} is simply a \ac{nn} that uses convolution instead of general matrix
multiplication in at least one of its layers. The main benefits of convolution in \ac{nn}s
are that it's dramatically more efficient in terms of memory requirements, it reduces
the amount of computation needed and it makes it possible to work with variable input
sizes.

A typical convolutional layer consists of three stages: a convolution stage, detector
stage and pooling stage. These can be implemented by individual layers. First, a
\textbf{kernel} is applied to the input data in positions separated by a stepsize. This
means that a linear activation function is fed by the matrix product of the input location
and the kernel's weight matrix. In the detector stage, the results are then run through
a non-linear activation, for example a \ac{relu}. Finally, a \textbf{pooling function}
is used to combine the results of multiple nearby outputs as the final output.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{cnn}
\caption*{Source: Recreated fig. 9.4 from Deep Learing~\cite{DEEP_LEARNING}}
\caption{Deeper layers in a convolutional network are connected to a larger window of the
         input data than shallow layers. This means that deeper layers can be indirectly
         connected to most or all of the input data even though their direct connections
         are sparse.~\cite{DEEP_LEARNING}}\label{fig:scon}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{cnn2}
\caption*{Source: Krizhevsky et.~al.~page 5.~\cite{NIPS_IMAGENET}}
\caption{The network used in~\cite{NIPS_IMAGENET} is a good example of a typical \ac{cnn}
         architecture. The first layer evaluates 11$\times$11$\times$3 kernels of pixels
         from the input and feeds into layers of increasing depth with the final
         convolutional layer working on 3$\times$3$\times$192 kernels. The final layers
         are fully connected with 4096 neurons and produce the networks output as a vector
         of probabilities over the 1000 trained subject classes.}\label{fig:cnn}
\end{figure}

\subsection{Stacked denoising autoencoders}
Autoencoders consist of an encoder, a decoder and a loss function. They first encode the
given data to a hidden representation and then reconstruct it while the loss function is
used in training. Reconstructing the exact input data is not useful and denoising
autoencoders avoid that by learning to encode a corrupted version of the input and
decode the result into useful features of the clean input. A stacked denoising autoencoder
utilizes a sequence of encoders followed by matching decoders trained this way.
Corrupted input data is only used to train the individual layers to find useful features
as a trained \ac{sdae} works on clean input.~\cite{SDAE}
\authorcomment{motivation for using sdaes?}
